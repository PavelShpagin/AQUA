\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\geometry{margin=1in}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{enumitem}

\title{\textbf{AQUA: Reproduction Report (Gold EN/DE/UA Only)}}
\author{Internal reproducibility note (publication-facing)}
\date{\today}

\begin{document}
\maketitle

\section{Scope}

This report reproduces the current evaluation pipeline on the \textbf{publication datasets only}:
\texttt{data/eval/gold\_en.csv}, \texttt{data/eval/gold\_de.csv}, \texttt{data/eval/gold\_ua.csv}.

We benchmark exactly three systems:
\begin{enumerate}[leftmargin=*]
    \item \textbf{Baseline}: single-call edit judge.
    \item \textbf{Agent}: multi-step edit agent.
\end{enumerate}

All metrics below are computed with \texttt{python -m gold\_eval.run\_gold} and are reported \textbf{on successful predictions only}; we also report success rate explicitly.

\section{Data provenance and filtering}

\subsection{Raw annotation sources (original 300 per language)}

The original annotation exports are:
\begin{itemize}[leftmargin=*]
    \item \textbf{EN}: \texttt{data/annotations/f3120198.csv} (\textbf{300} unique (src,tgt) pairs)
    \item \textbf{DE}: \texttt{data/annotations/f3120001.csv} (\textbf{300} unique (src,tgt) pairs)
    \item \textbf{UA}: \texttt{data/annotations/f3119729.csv} (\textbf{300} unique (src,tgt) pairs)
\end{itemize}

Each file contains multiple annotator rows per sentence pair.

\subsection{Gold construction (current filtered files)}

Gold construction is implemented in \texttt{processing/gold\_annotations.py}. The core steps are:
\begin{itemize}[leftmargin=*]
    \item \textbf{Label mapping}: map raw UI labels into \{TP, FP3, FP2, FP1\}.
    \item \textbf{Sentence label}: compute sentence-level label via majority vote (ties broken by severity FP1$>$FP2$>$FP3$>$TP) and apply TN/FN logic when src==tgt.
    \item \textbf{Edit-level aggregation}: aggregate edit labels across all rows sharing the same src (group-aware voting).
    \item \textbf{De-duplication}: enforce unique (src,tgt) rows.
\end{itemize}

The resulting in-repo Gold sizes are:
\begin{itemize}[leftmargin=*]
    \item \textbf{gold\_en}: 220 examples
    \item \textbf{gold\_de}: 240 examples
    \item \textbf{gold\_ua}: 252 examples
\end{itemize}

\textbf{Optional stricter filters} supported by \texttt{processing/gold\_annotations.py} (not applied to the current committed Gold files) include:
\texttt{--drop\_cant}, \texttt{--drop\_ties}, \texttt{--consistency}.

\section{Systems benchmarked}

\subsection{Baseline (edit baseline)}
\begin{itemize}[leftmargin=*]
    \item \textbf{Code}: \texttt{judges/edit/baseline.py}
    \item \textbf{Definition}: one LLM call using \texttt{EDIT\_LEVEL\_JUDGE\_PROMPT}, parse per-edit labels, then aggregate into sentence label (TP/FP1/FP2/FP3/TN/FN).
    \item \textbf{Backend}: \texttt{gpt-4o-mini}
    \item \textbf{Concurrency}: 10 workers (kept moderate to avoid API stress).
\end{itemize}

\subsection{Agent (edit agent)}
\begin{itemize}[leftmargin=*]
    \item \textbf{Code}: \texttt{judges/edit/agent.py}
    \item \textbf{Definition}: multi-step edit agent with \textbf{text tools} (spaCy cues) and a \textbf{rulebook tool} (offline, per-language local rulebook RAG). A grader pass exists but is disabled for these publication runs.
    \item \textbf{Backend}: \texttt{gpt-4.1-nano}
    \item \textbf{Concurrency}: 3 workers (conservative).
\end{itemize}

\section{Reproduction commands}

\subsection{Gold generation (from raw annotations)}
\begin{verbatim}
python3 processing/gold_annotations.py --output-dir data/eval
# optional stricter filtering:
# python3 processing/gold_annotations.py --output-dir data/eval --drop_cant 1 --drop_ties --consistency 2
\end{verbatim}

\subsection{Baseline runs (edit baseline)}
\begin{verbatim}
PYTHONPATH=$PWD python3 -m judges.edit.baseline --input data/eval/gold_en.csv --output data/results/repro_pub_edit_baseline_gpt-4o-mini_en/gold_en_labeled.csv --llm_backend gpt-4o-mini --lang en --workers 10
PYTHONPATH=$PWD python3 -m judges.edit.baseline --input data/eval/gold_de.csv --output data/results/repro_pub_edit_baseline_gpt-4o-mini_de/gold_de_labeled.csv --llm_backend gpt-4o-mini --lang de --workers 10
PYTHONPATH=$PWD python3 -m judges.edit.baseline --input data/eval/gold_ua.csv --output data/results/repro_pub_edit_baseline_gpt-4o-mini_ua/gold_ua_labeled.csv --llm_backend gpt-4o-mini --lang ua --workers 10
\end{verbatim}

\subsection{Agent runs}
\begin{verbatim}
PYTHONPATH=$PWD python3 judges/edit/agent.py --input data/eval/gold_en.csv --output data/results/repro_pub_agent_v3_nograder_gpt-4.1-nano_en/gold_en_labeled.csv --llm_backend gpt-4.1-nano --lang en --workers 3 --max_retries 1 --spacy on --rulebook on
PYTHONPATH=$PWD python3 judges/edit/agent.py --input data/eval/gold_de.csv --output data/results/repro_pub_agent_v3_nograder_gpt-4.1-nano_de/gold_de_labeled.csv --llm_backend gpt-4.1-nano --lang de --workers 3 --max_retries 1 --spacy on --rulebook on
PYTHONPATH=$PWD python3 judges/edit/agent.py --input data/eval/gold_ua.csv --output data/results/repro_pub_agent_v3_nograder_gpt-4.1-nano_ua/gold_ua_labeled.csv --llm_backend gpt-4.1-nano --lang ua --workers 3 --max_retries 1 --spacy on --rulebook on
\end{verbatim}

\subsection{Scoring}
\begin{verbatim}
python3 -m gold_eval.run_gold data/eval/gold_en.csv <pred.csv> --judge edit --method <baseline|agent> --lang en --llm_backend <backend>
python3 -m gold_eval.run_gold data/eval/gold_de.csv <pred.csv> --judge edit --method <baseline|agent> --lang de --llm_backend <backend>
python3 -m gold_eval.run_gold data/eval/gold_ua.csv <pred.csv> --judge edit --method <baseline|agent> --lang ua --llm_backend <backend>
\end{verbatim}

\section{Results (accuracy, success rate, cost)}

\textbf{Cost} is average \texttt{total\_cost\_usd} per non-error row (then scaled to \$/1K).\\
\textbf{Accuracy} is 6-class accuracy computed by \texttt{gold\_eval} on successful rows only.
\textbf{Binary accuracy} is computed by \texttt{gold\_eval} for the TP/FP decision (collapsed from the 6-class labels).

\begin{table}[h]
\centering
\begin{tabular}{@{}llrrrrr@{}}
\toprule
\textbf{Lang} & \textbf{Method} & \textbf{Acc (6cls)} & \textbf{Acc (bin)} & \textbf{Success} & \textbf{Errors} & \textbf{\$/1K} \\
\midrule
EN & Baseline (4o-mini, post-pass) & 0.583 & 0.686 & 211/220 & 9 & 0.277 \\
EN & Agent v2 (gated+verify, 4o-mini) & 0.559 & 0.636 & 220/220 & 0 & 0.126 \\
\midrule
DE & Baseline (4o-mini, post-pass) & 0.655 & 0.700 & 238/240 & 2 & 0.279 \\
DE & Agent v2 (gated+verify, 4o-mini) & 0.679 & 0.746 & 240/240 & 0 & 0.131 \\
\midrule
UA & Baseline (4o-mini, post-pass) & 0.681 & 0.726 & 251/252 & 1 & 0.267 \\
UA & Agent v2 (gated+verify, 4o-mini) & 0.659 & 0.679 & 252/252 & 0 & 0.141 \\
\bottomrule
\end{tabular}
\caption{Gold EN/DE/UA reproduction results (ensemble removed; binary accuracy included).}
\end{table}

\section{Notes / Known issues}
\begin{itemize}[leftmargin=*]
    \item \textbf{API conservatism}: all new runs used low worker counts (3--10) to avoid stressing the API key.
    \item \textbf{Why prior agent success was $<100\%$}: we traced this to \textbf{parsing/escaping edge cases} (quotes/backslashes inside alignment spans causing invalid JSON escapes). The agent was updated to be robust (escape repair, span extraction for insert/delete, strict JSON retry, and a last-resort single-label fallback), yielding \textbf{100\% success} on Gold EN/DE/UA.
    \item \textbf{Error rows}: baseline/ensemble still produce a small number of \texttt{Error} rows (timeouts / parse failures). We report success rate and compute accuracy on successful rows only.
    \item \textbf{Judge type alignment}: we report only \texttt{edit}-judge metrics (TP/FP1/FP2/FP3/TN/FN), matching the Gold label set.
    \item \textbf{Bottleneck for accuracy (not success)}: FP2 is rare and subtle; the dominant remaining errors are FP2 vs (TP/FP3) and ``style vs error'' boundaries. Simplified accuracy is consistently higher than strict 6-class accuracy.
\end{itemize}

\end{document}
